{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb217a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7efed435ebb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch libraries and modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, Sequential, Module, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.manual_seed(999) #for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06d0ceb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## data loaders\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "class DataCombiner(Dataset):\n",
    "    def __init__(self, ids, features):\n",
    "        self.ids = ids\n",
    "        self.features = features\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return (i, self.ids[i], self.features[i])\n",
    "    \n",
    "text_dim = 300\n",
    "column_dim = 900\n",
    "train_text_fraction = .32\n",
    "train_col_fraction = .32\n",
    "\n",
    "## read text list and column list\n",
    "def read_ids(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        csvf = csv.reader(f, delimiter='\\n')\n",
    "        return [r[0] for r in csvf]\n",
    "\n",
    "features_dir = '../features'\n",
    "datalake = 'mlopen' \n",
    "gt_path = '../inputs/mlopen-text-tables.gt'\n",
    "text_id = read_ids(os.path.join(features_dir, datalake + '-textids.list'))\n",
    "column_id = read_ids(os.path.join(features_dir, datalake + '-colids.list'))\n",
    "print(f'text_id len: {len(text_id)}, sample: {text_id[:2]}')\n",
    "print(f'col_id len: {len(column_id)}, sample: {column_id[:2]}')\n",
    "\n",
    "## load features\n",
    "text_f = torch.load(os.path.join(features_dir, datalake + '-textfeatures.pt'))\n",
    "column_f = torch.load(os.path.join(features_dir, datalake + '-columnfeatures.pt'))\n",
    "print(f'text_f len: {text_f.shape}, column_f len: {column_f.shape}')\n",
    "\n",
    "## get only a fraction for training\n",
    "train_text_id = text_id[:int(train_text_fraction * len(text_id))]\n",
    "train_column_id = column_id[:int(train_col_fraction * len(column_id))]\n",
    "train_text_f = text_f[:int(train_text_fraction * text_f.shape[0]), :]\n",
    "train_column_f = column_f[:int(train_col_fraction * column_f.shape[0]), :]\n",
    "print(f'sizes after filtering training data: {len(train_text_id)}, {len(train_column_id)}, {train_text_f.shape}, {train_column_f.shape}')\n",
    "\n",
    "## read labels\n",
    "def load_labels(label_path, text_id, column_id):\n",
    "    mat = torch.zeros(len(text_id), len(column_id), dtype=torch.float32)\n",
    "    with open(label_path, 'r') as f:\n",
    "        csvf = csv.reader(f)\n",
    "        last_text_key, last_text_index = -1, -1\n",
    "        for r in csvf:\n",
    "            try:\n",
    "                cid = column_id.index(r[1])\n",
    "                if r[0] == last_text_key: # no need to lookup again\n",
    "                    tid = last_text_index\n",
    "                else:\n",
    "                    tid = text_id.index(r[0])\n",
    "                    last_text_key = r[0]\n",
    "                    last_text_index = tid\n",
    "                mat[tid, cid] = float(r[2]) if len(r) > 2 else 1.\n",
    "            except:\n",
    "                continue\n",
    "    return mat\n",
    "    \n",
    "label_dir = '../column-labels'\n",
    "try:\n",
    "    label_mat = load_labels(os.path.join(label_dir, datalake + '-snorkel.lbl'), train_text_id, train_column_id).to(device)\n",
    "    print(f'label mat len: {label_mat.shape}, nonzero: {label_mat.count_nonzero()}')\n",
    "except RuntimeError as e:\n",
    "    print(e)\n",
    "    print(f'Exception: {torch.cuda.memory_allocated(device)}, \\\n",
    "      max allocated: {torch.cuda.max_memory_allocated(device)}, \\\n",
    "      reserved: {torch.cuda.memory_reserved(device)}, \\\n",
    "      max reserved: {torch.cuda.max_memory_reserved(device)}')\n",
    "\n",
    "\n",
    "train_text = DataCombiner(train_text_id, train_text_f)\n",
    "train_column = DataCombiner(train_column_id, train_column_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b82121",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loss functions\n",
    "def euclidean_dist(x,y):\n",
    "    m,n = x.size(0),y.size(0)\n",
    "    xx = torch.pow(x,2).sum(1,keepdim=True).expand(m,n)\n",
    "    yy = torch.pow(y,2).sum(dim=1,keepdim=True).expand(n,m).t()\n",
    "    dist = xx + yy\n",
    "    dist.addmm_(1,-2,x,y.t())\n",
    "    dist = dist.clamp(min=1e-12).sqrt()\n",
    "    return dist\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=0.1, neg_weight=1., normalize_feature=True):\n",
    "        super(TripletLoss,  self).__init__()\n",
    "        self.margin = margin\n",
    "        self.neg_weight = neg_weight\n",
    "        self.normalize_feature = normalize_feature\n",
    "\n",
    "    ## emb1: text_bsz*100 features, emb2: column_bsz*100 features, \n",
    "    ## label_mat: text_bsz * column_bsz indicates the magnitude of positive relation between each pair\n",
    "    ## mask1: text_bsz*1 telling to ignore rows where either all labels are zeros or all labels are non-zeros \n",
    "    def forward(self, emb1, emb2, label_mat, mask1):\n",
    "        if self.normalize_feature:\n",
    "            emb1 = emb1.sigmoid() #F.normalize(emb1)\n",
    "            emb2 = emb2.sigmoid() #F.normalize(emb2)\n",
    "        mat_dist = euclidean_dist(emb1, emb2) ## (text_bsz, column_bsz)\n",
    "        N = mat_dist.size(0) ## ==text_bsz\n",
    "        eligible = torch.count_nonzero(mask1)\n",
    "        if eligible == 0:\n",
    "            tqdm.write('Found all zeros mask')\n",
    "            return (torch.tensor(0), torch.tensor(1))\n",
    "        \n",
    "        # sampling positives, each will be brought closer to anchor\n",
    "        positives = mask1 * mat_dist * label_mat ## (text_bsz, column_bsz)\n",
    "        dist_ap = torch.sum(positives, dim=1) ## (text_bsz, 1)\n",
    "        \n",
    "        # sampling negatives, hard sampling, the nearest one will be pushed beyond margin\n",
    "        negatives, neg_indices = torch.sort( mat_dist + 100000.0 * (label_mat + 1 - mask1), dim = 1, descending=False )\n",
    "        dist_an = negatives[:,0]\n",
    "        \n",
    "        #tqdm.write(f'label_mat: {label_mat}, mat_dist: {mat_dist}, \\\n",
    "        #           positives: {positives}, dist_ap: {dist_ap}, negatives: {negatives}, dist_an: {dist_an}')\n",
    "        # aggregate loss\n",
    "        loss = torch.sum(dist_ap \n",
    "                          + self.neg_weight * torch.max(torch.zeros_like(dist_an), self.margin - dist_an)) / eligible\n",
    "        prec = (dist_an.data > dist_ap.data).sum() * 1.0 / N\n",
    "        #tqdm.write(f'Loss: {loss}, Prec: {prec}')\n",
    "        return (loss, prec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67c619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Embedding model\n",
    "\n",
    "class EncoderNet(nn.Module):\n",
    "    def __init__(self, ip=1000, op=100, hidden1=200, hidden2=200):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        self.net = nn.Sequential( #sequential operation\n",
    "            nn.Linear(ip, hidden1), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden1),\n",
    "            nn.Linear(hidden1, hidden2), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden2),\n",
    "            #nn.Dropout(p=0.2),  \n",
    "            nn.Linear(hidden2, op)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.net(X)\n",
    "\n",
    "text_enet = (EncoderNet(text_dim)).to(device)\n",
    "column_enet = (EncoderNet(column_dim)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe416103",
   "metadata": {},
   "outputs": [],
   "source": [
    "## hyper parameters\n",
    "text_bsz = int(32)\n",
    "column_bsz = int(32)\n",
    "\n",
    "epochs = 500\n",
    "stepsPerEpoch = int(26)\n",
    "\n",
    "## triplet loss parameters\n",
    "triplet_margin = 0.1 # Lower the better, but not below 0.1. Higher values improve results when a large fraction of data is used for training\n",
    "neg_weight = 1 # Lower values lead to no learning, but can go higher for better recall.\n",
    "\n",
    "# Optimizer\n",
    "encoder_learning_rate = 0.0001 # triplet loss doesn't converge with 0.001, formula from Dino: lr=0.0005*bsz/256\n",
    "encoder_decay_rate = 0.001 # For regularization, use 0.001 when using triplet loss\n",
    "\n",
    "parameters = set()\n",
    "for net_ in text_enet, column_enet:\n",
    "  parameters |= set(net_.parameters())\n",
    "enet_optimizer = torch.optim.Adam(parameters, lr=encoder_learning_rate, weight_decay=encoder_decay_rate, amsgrad=True)\n",
    "\n",
    "## loss function for semantics transfer across modalities\n",
    "triplet_loss_fn = TripletLoss(margin=triplet_margin, neg_weight=neg_weight, normalize_feature=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9598984f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Training and validation module\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dataset load\n",
    "train_text_loader = DataLoader(train_text, batch_size=text_bsz, shuffle=True, drop_last=False)\n",
    "train_column_loader = DataLoader(train_column, batch_size=column_bsz, shuffle=True, drop_last=False)\n",
    "\n",
    "# Training metrics\n",
    "triplet_losses = []\n",
    "triplet_accuracies = []\n",
    "\n",
    "# eval metrics\n",
    "eval_result_tuples = []\n",
    "\n",
    "for epoch in tqdm(range(0, epochs)):\n",
    "\n",
    "    ## validation: need one validation run with random weights, so starting off with it\n",
    "    if epoch % 10 == 0:\n",
    "        for _net in text_enet, column_enet:\n",
    "            _net.eval()\n",
    "        \n",
    "        ## write embeddings for all features to disk\n",
    "        t_emb = text_enet(text_f.to(device)).detach().cpu().numpy()\n",
    "        c_emb = column_enet(column_f.to(device)).detach().cpu().numpy()\n",
    "        op_dir = '/tmp'\n",
    "        np.save(os.path.join(op_dir, datalake + '-' + str(epoch) + '-trainedtext.npy'), t_emb)\n",
    "        np.save(os.path.join(op_dir, datalake + '-' + str(epoch) + '-trainedcolumns.npy'), c_emb)\n",
    "        \n",
    "        ## fetch results\n",
    "        %run ../evaluate_trained.py -f$op_dir -d $datalake -i$epoch -g$gt_path\n",
    "        print(eval_results)\n",
    "        eval_result_tuples.extend(eval_results)\n",
    "        \n",
    "        for _net in text_enet, column_enet:\n",
    "            _net.train()\n",
    "\n",
    "    ## training\n",
    "    epoch_loss = 0.\n",
    "    epoch_acc = 0.\n",
    "\n",
    "    # start mini-batches\n",
    "    step_count = 0\n",
    "    train_text_iterator = iter(train_text_loader)\n",
    "    train_column_iterator = iter(train_column_loader)\n",
    "    while (step_count < stepsPerEpoch):\n",
    "        # read next batch\n",
    "        try:\n",
    "            t_idx, t_id, t_f = next(train_text_iterator)\n",
    "        except StopIteration:\n",
    "            train_text_iterator = iter(train_text_loader)\n",
    "            t_idx, t_id, t_f = next(train_text_iterator)\n",
    "            \n",
    "        try:\n",
    "            c_idx, c_id, c_f = next(train_column_iterator)\n",
    "        except StopIteration:\n",
    "            train_column_iterator = iter(train_column_loader)\n",
    "            c_idx, c_id, c_f = next(train_column_iterator)\n",
    "        \n",
    "        # to device\n",
    "        t_idx = t_idx.to(device)\n",
    "        t_f = t_f.to(device)\n",
    "        c_idx = c_idx.to(device)\n",
    "        c_f = c_f.to(device)\n",
    "        \n",
    "        # construct label matrix and mask array\n",
    "        label_batch = label_mat[t_idx, :]\n",
    "        label_batch = label_batch[:, c_idx]\n",
    "        mask = torch.count_nonzero(label_batch, dim=1)\n",
    "        mask = torch.where((mask>0) & (mask<column_bsz), 1, 0).unsqueeze(-1)\n",
    "\n",
    "        # embeddings\n",
    "        t_emb = text_enet(t_f)\n",
    "        c_emb = column_enet(c_f)\n",
    "        \n",
    "        # loss\n",
    "        loss, acc = triplet_loss_fn(t_emb, c_emb, label_batch, mask)\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "        # step\n",
    "        if loss > 0:\n",
    "            enet_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            enet_optimizer.step()\n",
    "\n",
    "        # increment step count\n",
    "        step_count += 1\n",
    "        \n",
    "    # aggregate metrics\n",
    "    tqdm.write(f'Epoch {epoch}: loss= {epoch_loss/step_count}, accuracy= {epoch_acc/step_count}')\n",
    "    triplet_losses.append(epoch_loss/step_count)\n",
    "    triplet_accuracies.append(epoch_acc/step_count)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a9e388",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot training performance\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "%matplotlib inline\n",
    "set_matplotlib_formats('svg')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(triplet_losses, label='Loss')\n",
    "plt.title('Training losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(triplet_accuracies, label='Training Accuracy')\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9be9e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## validation plots\n",
    "\n",
    "## save eval result\n",
    "def write_csv(fp, result):\n",
    "    with open(fp, 'w') as f:\n",
    "        csvf = csv.writer(f)\n",
    "        for r in result:\n",
    "            csvf.writerow(r)\n",
    "\n",
    "print('final result: ')\n",
    "for t in eval_result_tuples[:-9]:\n",
    "    res = ','.join([str(x) for x in t])\n",
    "    frac = \"{:.2f}\".format(train_text_fraction * train_col_fraction)\n",
    "    print(str(frac) + ',' + res)\n",
    "\n",
    "# R-precision\n",
    "eval_rprecision = [t[3] for t in eval_result_tuples if t[1] == 8]\n",
    "plt.figure()\n",
    "plt.plot(eval_rprecision)\n",
    "plt.title('R-precision')\n",
    "plt.xlabel('Epochs * 10')\n",
    "plt.ylabel('R-Precision')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# scatter plot\n",
    "prec_values = [t[3] for t in eval_result_tuples]\n",
    "rec_values = [t[4] for t in eval_result_tuples]\n",
    "colors = [t[0] for t in eval_result_tuples]\n",
    "plt.figure()\n",
    "sc = plt.scatter(prec_values, rec_values, c=colors, cmap=\"RdYlGn\")\n",
    "plt.colorbar(sc)\n",
    "plt.title('P-R trade-offs with training')\n",
    "plt.xlabel('Precision')\n",
    "plt.ylabel('Recall')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec54e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "## export trained vectors to disk\n",
    "t_emb = text_enet(text_f.to(device)).detach().cpu().numpy()\n",
    "c_emb = column_enet(column_f.to(device)).detach().cpu().numpy()\n",
    "\n",
    "print(f'Writing learned features of shape: {t_emb.shape} and {c_emb.shape}')\n",
    "op_dir = '../features'\n",
    "np.save(os.path.join(op_dir, datalake + '-trainedtext.npy'), t_emb)\n",
    "np.save(os.path.join(op_dir, datalake + '-trainedcolumns.npy'), c_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75cd436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4290c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "t_emb = text_enet(text_f.to(device))\n",
    "c_emb = column_enet(column_f.to(device))\n",
    "\n",
    "dist = euclidean_dist(t_emb, c_emb)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd55866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bae9a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## to help fit label matrix\n",
    "print(f'Before emptying: memory allocated: {torch.cuda.memory_allocated(device)}, \\\n",
    "      max allocated: {torch.cuda.max_memory_allocated(device)}, \\\n",
    "      reserved: {torch.cuda.memory_reserved(device)}, \\\n",
    "      max reserved: {torch.cuda.max_memory_reserved(device)}')\n",
    "torch.cuda.empty_cache()\n",
    "print(f'After emptying: memory allocated: {torch.cuda.memory_allocated(device)}, \\\n",
    "      max allocated: {torch.cuda.max_memory_allocated(device)}, \\\n",
    "      reserved: {torch.cuda.memory_reserved(device)}, \\\n",
    "      max reserved: {torch.cuda.max_memory_reserved(device)}')\n",
    "\n",
    "print(torch.__version__)\n",
    "crow_indices = torch.tensor([0, 2, 4])\n",
    "col_indices = torch.tensor([0, 1, 0, 1])\n",
    "values = torch.tensor([1, 2, 3, 4])\n",
    "csr = torch._sparse_csr_tensor(crow_indices, col_indices, values, dtype=torch.double)\n",
    "print(csr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
